# InstructLab Teacher Model Validation

Authors: Jenny Yi, Oleg Silkin, Aakanksha Duggal

## Overview

Validation of the teacher model happens on two distinct levels:

1. **Data-level:** Assessing the quality of synthetic data generated by the teacher, independent of any training outcomes.
2. **Model-level**: Measuring the performance of a student model trained on above data to evaluate the teacher's effectiveness.

On the **data level**, the teacher needs to meet a set of checks which prove its efficacy. These checks include (but are not limited to):

- Does the generated data match the quality of existing models?
- Will the new data create conflicts with existing student models?
- Can we guarantee quality of data in domain-specific scenarios?

On the **model level**, the resulting fine-tuned student model is then evaluated across three dimensions: general knowledge and reasoning performance, memorization of newly introduced data (Memory Mode), and effectiveness in retrieval-augmented generation (RAG evaluation). Data collected in this validation process is surfaced through the [Red Hat Model Validation Program](https://www.redhat.com/en/products/ai/validated-models).

## Data-Level Validation

Here is a breakdown of full list of datasets used in the training process:

- **Pre-computed dataset:** A subset of data from the granite-3.1-8b-instruct training set which we use to create a final instruct-tuned model for customers to use.
- **RAFT samples:** To boost the model's performance on RAG scenarios, we generate samples where the model is shown the domain-specific data along with the relevant context and some distractor documents so we can tune its accuracy on retrieval tasks.
- **Domain-specific data**: We generate data from a customer's domain-specific knowledge set. *Currently, the data sources are technically public but curated to simulate a focused scrape of a specific domain (e.g., Red Hat's docs site), allowing the model to respond to more targeted, organization-specific questions.*

**Domain-specific data** will be generated by the new teacher model using [InstructLab's Synthetic Data Generation (SDG) pipeline](https://github.com/instructlab/sdg), serving as an indicator of the model's effectiveness as a teacher. The teacher is expected to generate a comprehensive set of training examples in question and answer pairs format based on seed examples from the original source.

The SDG validation process is as follows:

- A chat template for the new teacher model is registered in the prompt registry like [this](https://github.com/instructlab/sdg/blob/5d14ebd224199a0c78a5f83abb6dd400d4f99521/src/instructlab/sdg/prompts.py#L27).
- Ensure the model family's prompt structures are followed in the configs properly.
  - For example: LLaMA models handle plain text well, while Mixtral models require a unique structured format. Mixtral templates may work for LLaMA, but not vice versa.
- Validate that the ilab data generate runs with no errors and outputs the generated dataset.
  - Since there are 3 different kinds of leaf node inputs, at least one knowledge, one grounded skill and one freeform skill is validated.
  - It's recommended to start with a small, isolated sample to catch issues like gibberish text caused by incorrect templates or prompt structures.

## Model-Level Validation

To evaluate at the model-level, the student model is trained by the data generated by the new teacher. The student model goes through this training pipeline:

### Phase 1

The model is trained purely on knowledge samples from the **domain-specific data**. We employ a high learning rate (2e-5) and unmask the knowledge documents so that the model is taught to memorize their contents when prompted with domain-specific questions. We also utilize a smaller batch size (128) in order for the model to have the best chance of accurately capturing these (usually) smaller datasets.

The data used in this stage of training is: **Domain-specific knowledge samples**

### Phase 2

Due to the training parameters of the first stage, the model has likely forgotten critical aspects of instruct-tuning which enable it to do so well as a conversational model. The second stage addresses this by training on larger batches (3840) of high-quality instruction samples using a smaller learning rate (6e-6).

At this stage of training, we use the following samples: **RAFT samples, Pre-computed dataset**

After **Phase 2** of training, we evaluate all of the checkpoints on OpenLLM Leaderboard v2 to find the best performer model. This checkpoint becomes the "winner" and is then used as the target for all subsequent evaluations.

## Evaluations

The fine-tuned model (named granite-3.1-8B-lab) is evaluated by a set of benchmarks aimed at three purposes:

- **General accuracy: OpenLLM Leaderboard v2 is used as a "catch-all" evaluation and [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) is used as the framework**
  - **MMLU-Pro**: Broad-range set of difficult questions designed to test a model's understanding of language.
  - **BBH**: A suite of hard reasoning tasks curated to evaluate complex multi-step problem-solving abilities.
  - **GPQA**: 448 challenging questions designed to be google-proof, targeting deep factual and conceptual understanding.
  - **IFEVAL**: Measures instruction-following capabilities by evaluating how well a model adheres to user commands in diverse tasks.
  - **MuSR**: Assesses multilingual scientific reasoning by testing models on questions across various scientific domains in multiple languages.
- **Teacher model quality: "Memory Mode" QA is used to measure how well the student learned from the generated data**
- **Realistic teacher model quality: RAG Eval gets used to most accurately represent teacher model impact on RAG scenarios**
